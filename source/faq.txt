===
FAQ
===

.. default-domain:: mongodb

How can I achieve data locality?
--------------------------------

For any MongoDB deployment, the Mongo Spark Connector sets the
preferred location for an RDD to be where the data is:

- For a non sharded system, it sets the preferred location to be the
  hostname(s) of the standalone or the replica set.

- For a sharded system, it sets the preferred location to be the
  hostname(s) of the shards.

To achieve full data locality,

- Ensure there is a Spark Worker on one of the hosts for non-sharded
  system or one per shard for sharded systems.

- Use a :readmode:`nearest` read preference to read from the local
  :program:`mongod`.

- For a sharded cluster, you should have a :program:`mongos` on the
  same nodes and use :setting:`spark.mongodb.input.localThreshold`
  configuration to connect to the nearest :program:`mongos`.


How do I interact with Spark Streams?
-------------------------------------

Spark streams can be considered as a potentially infinite source of
RDDs. Therefore, anything you can do with an RDD, you can do with the
results of a Spark Stream.

For an example, see :mongo-spark:`SparkStreams.scala
</blob/master/examples/src/test/scala/tour/SparkStreams.scala>`
